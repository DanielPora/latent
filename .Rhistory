}
## f) We use the above code and the results matrix to calculate power. Recall that the power is
##    the probability of rejecting the Null hypothesis, given a specific effect size.
##    We can approximate this by calculating the proportion of simulated datasets,
##    where the effect comes out significant, i.e. below 0.05.
##    Calculate the power based on the simulations for all three effects of interest
##    (i.e., predA, predB and the interaction) individually.
sum(results[,2]<0.05)/1000
sum(results[,3]<0.05)/1000
sum(results[,4]<0.05)/1000
## g) How does power change when you decrease your alpha level to 0.01?
sum(results[,2]<0.01)/1000
sum(results[,3]<0.01)/1000
sum(results[,4]<0.01)/1000
sim = 1000 # number of simulations
n   = 100  # number of participants in each simulation
## results matrix
results = matrix(nrow=sim, ncol=4)
colnames(results) <- c("Intercept", "predA", "predB", "interaction")
for(i in c(1:sim)){
predA    <- rnorm(n, 80, 20)
predB    <- rnorm(n, 30, 30)
interact <- 0.08*(predA*predB)
error    <- rnorm(n, 0, 50)
resp     <- 42 + 0.2*predA - 5.3*predB + interact + error
d        <- data.frame(predA, predB, resp)
m1       <- lm(resp~predA*predB, data=d)
## store the resulting p-values in the results matrix
results[i,] = summary(m1)$coefficients[,4]
}
sum(results[,2]<0.05)/1000
sum(results[,3]<0.05)/1000
sum(results[,4]<0.05)/1000
## g) How does power change when you decrease your alpha level to 0.01?
sum(results[,2]<0.01)/1000
sum(results[,3]<0.01)/1000
sum(results[,4]<0.01)/1000
n = 50
for(i in c(1:sim)){
predA    <- rnorm(n, 80, 20)
predB    <- rnorm(n, 30, 30)
interact <- 0.08*(predA*predB)
error    <- rnorm(n, 0, 50)
resp     <- 42 + 0.2*predA - 5.3*predB + interact + error
d        <- data.frame(predA, predB, resp)
m1       <- lm(resp~predA*predB, data=d)
## store the resulting p-values in the results matrix
results[i,] = summary(m1)$coefficients[,4]
}
sum(results[,2]<0.05)/1000
sum(results[,3]<0.05)/1000
sum(results[,4]<0.05)/1000
sim = 1000 # number of simulations
n   = 100  # number of participants in each simulation
## results matrix
results = matrix(nrow=sim, ncol=4)
colnames(results) <- c("Intercept", "predA", "predB", "interaction")
for(i in c(1:sim)){
predA    <- rnorm(n, 80, 20)
predB    <- rnorm(n, 30, 30)
interact <- 0.08*(predA*predB)
error    <- rnorm(n, 0, 50)
resp     <- 42 + 0.2*predA - 5.3*predB + interact + error
d        <- data.frame(predA, predB, resp)
m1       <- lm(resp~predA*predB, data=d)
## store the resulting p-values in the results matrix
results[i,] = summary(m1)$coefficients[,4]
}
mean(results[,2] < 0.05)
mean(results[,3] < 0.05)
mean(results[,4] < 0.05)
mean(results[,2] < 0.01)
mean(results[,3] < 0.01)
mean(results[,4] < 0.01)
sim = 1000
n = 50
results = matrix(nrow=sim, ncol=4)
colnames(results) <- c("Intercept", "predA", "predB", "interaction")
for(i in c(1:sim)){
predA    <- rnorm(n, 80, 20)
predB    <- rnorm(n, 30, 30)
interact <- 0.08*(predA*predB)
error    <- rnorm(n, 0, 50)
resp     <- 42 + 0.2*predA - 5.3*predB + interact + error
d        <- data.frame(predA, predB, resp)
m1       <- lm(resp~predA*predB, data=d)
results[i,] = summary(m1)$coefficients[,4]
}
mean(results[,2] < 0.05)
mean(results[,3] < 0.05)
mean(results[,4] < 0.05)
sim = 1000 # number of simulations
n   = 100  # number of participants in each simulation
## results matrix
results = matrix(nrow=sim, ncol=4)
colnames(results) <- c("Intercept", "predA", "predB", "interaction")
for(i in c(1:sim)){
predA    <- rnorm(n, 80, 20)
predB    <- rnorm(n, 30, 30)
interact <- 0.08*(predA*predB)
error    <- rnorm(n, 0, 50)
resp     <- 42 + 0.2*predA - 5.3*predB + interact + error
d        <- data.frame(predA, predB, resp)
m1       <- lm(resp~predA*predB, data=d)
## store the resulting p-values in the results matrix
results[i,] = summary(m1)$coefficients[,4]
}
sum(results[,"predA"] < 0.05) / nrow(results)       # power of predA
sum(results[,"predB"] < 0.05) / nrow(results)       # power of predB
sum(results[,"interaction"] < 0.05) / nrow(results) # power of interaction
sim = 1000 # number of simulations
n   = 100  # number of participants in each simulation
## results matrix
results = matrix(nrow=sim, ncol=4)
colnames(results) <- c("Intercept", "predA", "predB", "interaction")
for(i in c(1:sim)){
predA    <- rnorm(n, 80, 20)
predB    <- rnorm(n, 30, 30)
interact <- 0.08*(predA*predB)
error    <- rnorm(n, 0, 50)
resp     <- 42 + 0.2*predA - 5.3*predB + interact + error
d        <- data.frame(predA, predB, resp)
m1       <- lm(resp~predA*predB, data=d)
## store the resulting p-values in the results matrix
results[i,] = summary(m1)$coefficients[,4]
}
library(dplyr)
nrow(filter(as.data.frame(results), predA < 0.05))/sim # 0.093
nrow(filter(as.data.frame(results), predB < 0.05))/sim # 1
nrow(filter(as.data.frame(results), interaction < 0.05))/sim # 1
## g) How does power change when you decrease your alpha level to 0.01?
nrow(filter(as.data.frame(results), predA < 0.01))/sim # 0.021
nrow(filter(as.data.frame(results), predB < 0.01))/sim # 1
nrow(filter(as.data.frame(results), interaction < 0.01))/sim # 1
## g) How does power change when you decrease your alpha level to 0.01?
sum(results[,2]<0.01)/1000
sim = 1000 # number of simulations
n   = 100  # number of participants in each simulation
## results matrix
results = matrix(nrow=sim, ncol=4)
colnames(results) <- c("Intercept", "predA", "predB", "interaction")
for(i in c(1:sim)){
predA    <- rnorm(n, 80, 20)
predB    <- rnorm(n, 30, 30)
interact <- 0.08*(predA*predB)
error    <- rnorm(n, 0, 50)
resp     <- 42 + 0.2*predA - 5.3*predB + interact + error
d        <- data.frame(predA, predB, resp)
m1       <- lm(resp~predA*predB, data=d)
## store the resulting p-values in the results matrix
results[i,] = summary(m1)$coefficients[,4]
}
## g) How does power change when you decrease your alpha level to 0.01?
sum(results[,2]<0.01)/1000
sum(results[,3]<0.01)/1000
sum(results[,4]<0.01)/1000
sim = 1000 # number of simulations
n   = 100  # number of participants in each simulation
## results matrix
results = matrix(nrow=sim, ncol=4)
colnames(results) <- c("Intercept", "predA", "predB", "interaction")
for(i in c(1:sim)){
predA    <- rnorm(n, 80, 20)
predB    <- rnorm(n, 30, 30)
interact <- 0.08*(predA*predB)
error    <- rnorm(n, 0, 50)
resp     <- 42 + 0.2*predA - 5.3*predB + interact + error
d        <- data.frame(predA, predB, resp)
m1       <- lm(resp~predA*predB, data=d)
## store the resulting p-values in the results matrix
results[i,] = summary(m1)$coefficients[,4]
}
## f) We use the above code and the results matrix to calculate power. Recall that the power is
##    the probability of rejecting the Null hypothesis, given a specific effect size.
##    We can approximate this by calculating the proportion of simulated datasets,
##    where the effect comes out significant, i.e. below 0.05.
##    Calculate the power based on the simulations for all three effects of interest
##    (i.e., predA, predB and the interaction) individually.
library(dplyr)
nrow(filter(as.data.frame(results), predA < 0.05))/sim # 0.093
nrow(filter(as.data.frame(results), predB < 0.05))/sim # 1
nrow(filter(as.data.frame(results), interaction < 0.05))/sim # 1
## g) How does power change when you decrease your alpha level to 0.01?
nrow(filter(as.data.frame(results), predA < 0.01))/sim # 0.021
nrow(filter(as.data.frame(results), predB < 0.01))/sim # 1
nrow(filter(as.data.frame(results), interaction < 0.01))/sim # 1
simh = 1000 # number of simulations
nh   =50  # number of participants in each simulation
## results matrix
resultsh = matrix(nrow=simh, ncol=4)
colnames(resultsh) <- c("Intercept", "predA", "predB", "interaction")
for(i in c(1:sim)){
predA    <- rnorm(nh, 80, 20)
predB    <- rnorm(nh, 30, 30)
interact <- 0.08*(predA*predB)
error    <- rnorm(nh, 0, 50)
resp     <- 42 + 0.2*predA - 5.3*predB + interact + error
d        <- data.frame(predA, predB, resp)
m1       <- lm(resp~predA*predB, data=d)
## store the resulting p-values in the results matrix
resultsh[i,] = summary(m1)$coefficients[,4]
}
nrow(filter(as.data.frame(resultsh), predA < 0.05))/sim # 0.059
nrow(filter(as.data.frame(resultsh), predB < 0.05))/sim # 0.986
nrow(filter(as.data.frame(resultsh), interaction < 0.05))/sim # 0.996
library(dplyr)
library(ggplot2)
library(rstudioapi)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(ggcorrplot)
library(tidyverse)
library('corrr')
#install.packages("tidyverse")
library("FactoMineR")
library("factoextra")
dat_tar <- read.csv("data_s3_target.csv")
dat_id <- read.csv("data_s3_ID.csv")
# clean_F.csv concerns only columns that are relevant for the analysis
clean <- read.csv("clean_F.csv")
# Using a generative adversarial network (CTGAN) to generate synthetic data did
# not proof to be viable. It was not able to pick up several important
# very strong correlations between columns. E.g. the one-hot encoding between
# own.cod and other.cod, target and response object have to be of the same test
# type and so on. An example of this synthetic data can be found in synth1000.
# It's planned to train this model again on a much more sparse predictor space
# without the strong correlations and account for the multiple measurements
# coming from the same participant
synth1000 <- read.csv("synth1000.csv")
# 1. Specific clusters regarding to the tests
subj_tests <- clean %>%
select("workerID", "aq_score_subset", "opt_score", "stroop_difference")%>%
distinct()
# 1.1. Normalizing the tests data
subj_tests_norm <- as.data.frame(scale(subj_tests[2:4]))
subj_tests_norm <- subj_tests_norm%>%
mutate("workerID" = subj_tests$workerID)
data_normalized <- subj_tests_norm %>%
select("workerID", "aq_score_subset", "opt_score", "stroop_difference")
tests_norm <- data_normalized[,-1]
rownames(tests_norm) <- data_normalized[,1]
ggplot(stack(tests_norm), aes(x = ind, y = values)) +
stat_boxplot(geom = "errorbar", width = 0.25) +
labs(x="Test", y="Normalized Value") +
geom_boxplot()
# 1.2 Plotting densities of each test
ggplot(tests_norm, aes(aq_score_subset))+
geom_density()
ggplot(tests_norm, aes(opt_score))+
geom_density()
ggplot(tests_norm, aes(stroop_difference))+
geom_density()
# 1.3 Scatterplots of pairwise tests
ggplot(tests_norm, aes(aq_score_subset, opt_score))+
geom_point()
ggplot(tests_norm, aes(aq_score_subset, stroop_difference))+
geom_point()
ggplot(tests_norm, aes(stroop_difference, opt_score))+
geom_point()
corr_matrix <- cor(tests_norm)
corr_matrix
# 2.1. Identify problems for same perspective task
problems <- clean %>%
filter(clean$accuracy == "0")
# Front-Back problems
fb_p <- problems %>%
filter(problems$targetPos == "F" |problems$targetPos == "B")
# Left-Right problems
lr_p <- problems %>%
filter(problems$targetPos == "L" |problems$targetPos == "R")
nrow(fb_p)
nrow(lr_p)
clean %>%
count(perspective)
worker_fb_p <- fb_p %>%
count(workerID, sort = TRUE)
worker_fb <- clean %>%
filter(perspective == "same", targetPos == "F" | targetPos == "B") %>%
count(workerID)
diff <- left_join(worker_fb_p, worker_fb, by="workerID", suffix=c("_p", "_all"))
diff %>%
mutate(prop = round(n_p/n_all,2))%>%
arrange(desc(prop))
library(dplyr)
library(ggplot2)
library(rstudioapi)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(ggcorrplot)
library(tidyverse)
library('corrr')
#install.packages("tidyverse")
library("FactoMineR")
library("factoextra")
dat_tar <- read.csv("data_s3_target.csv")
dat_id <- read.csv("data_s3_ID.csv")
# clean_F.csv concerns only columns that are relevant for the analysis
clean <- read.csv("clean_F.csv")
# Preconsideration
# Using a generative adversarial network (CTGAN) to generate synthetic data did
# not proof to be viable. It was not able to pick up several important
# very strong correlations between columns. E.g. the one-hot encoding between
# own.cod and other.cod, target and response object have to be of the same test
# type and so on. An example of this synthetic data can be found in synth1000.
# It's planned to train this model again on a much more sparse predictor space
# without the strong correlations and account for the multiple measurements
# coming from the same participant
synth1000 <- read.csv("synth1000.csv")
# In order to generate authentic data a good understanding of the data and it's
# distributions is necessary
# 1. Specific clusters regarding to the tests
subj_tests <- clean %>%
select("workerID", "aq_score_subset", "opt_score", "stroop_difference")%>%
distinct()
# 1.1. Normalizing the tests data
subj_tests_norm <- as.data.frame(scale(subj_tests[2:4]))
subj_tests_norm <- subj_tests_norm%>%
mutate("workerID" = subj_tests$workerID)
data_normalized <- subj_tests_norm %>%
select("workerID", "aq_score_subset", "opt_score", "stroop_difference")
tests_norm <- data_normalized[,-1]
rownames(tests_norm) <- data_normalized[,1]
ggplot(stack(tests_norm), aes(x = ind, y = values)) +
stat_boxplot(geom = "errorbar", width = 0.25) +
labs(x="Test", y="Normalized Value") +
geom_boxplot()
# 1.2 Plotting densities of each test
ggplot(tests_norm, aes(aq_score_subset))+
geom_density()
ggplot(tests_norm, aes(opt_score))+
geom_density()
ggplot(tests_norm, aes(stroop_difference))+
geom_density()
# 1.3 Scatterplots of pairwise tests
ggplot(tests_norm, aes(aq_score_subset, opt_score))+
geom_point()
ggplot(tests_norm, aes(aq_score_subset, stroop_difference))+
geom_point()
ggplot(tests_norm, aes(stroop_difference, opt_score))+
geom_point()
corr_matrix <- cor(tests_norm)
corr_matrix
# 1.4 Summary
# The scatter plots don't hint at some correlation, using a correlation
# matrix to confirm.
# Density of OPT shows a bi-modal distribution. Suggesting that participants
# can be grouped in two optical perspective related groups.
# There doesn't seem to be more of a structure. PCA was done in a separated
# analysis, but without any insightful result.
# 2. Perspective taking preferences
# 2.1. Identify problems for same perspective task
problems <- clean %>%
filter(clean$accuracy == "0")
# Front-Back problems
fb_p <- problems %>%
filter(problems$targetPos == "F" |problems$targetPos == "B")
# Left-Right problems
lr_p <- problems %>%
filter(problems$targetPos == "L" |problems$targetPos == "R")
nrow(fb_p)
nrow(lr_p)
clean %>%
count(perspective)
worker_fb_p <- fb_p %>%
count(workerID, sort = TRUE)
worker_fb <- clean %>%
filter(perspective == "same", targetPos == "F" | targetPos == "B") %>%
count(workerID)
diff <- left_join(worker_fb_p, worker_fb, by="workerID", suffix=c("_p", "_all"))
diff %>%
mutate(prop = round(n_p/n_all,2))%>%
arrange(desc(prop))
# 2.2 Summary
# In 1404 same perspective experiments occurred 1 left-right confusion (>0.001),
# which is neglect able, and 74 front-back confusions (=0.05)
# This suggest that in about 5% of the cases for a "different" perspective
# front-back scenario the own-other perspective classification is not correct.
# In particular we can see that half of the participants who confused front-back
# did it systematically. Therefore it's suggested to invert the interpretation
# for their own.cod and other.cod entries
# 3 Individual perspective preference
# 3.1 Relevant here are the "different" perspective scenarios
# Using front-back, left-right...
fb <- clean %>%
filter(perspective == "different", targetPos == "F" | targetPos == "B")
lr <- clean %>%
filter(perspective == "different", targetPos == "L" | targetPos == "R")
# ... and all scenarios together to analyse the perspective preference by subject
# own_tendency describes here whether the participant preferred the egocentric
# perspective versus the othercentric perspective.
# 1 = always egocentric, 0 = 50/50, -1 = always othercentric
by_subj_diff <- clean %>%
filter(perspective == "different") %>%
group_by(workerID) %>%
summarise(respTime_mean = mean(respTime), respTime_sd = sd(respTime),
own_sum_diff = sum(own.cod), other_sum_diff = sum(other.cod),
own_tendency = (sum(own.cod)-sum(other.cod))/(sum(own.cod)+sum(other.cod)),
aq_score = mean(aq_score_subset),
opt_score = mean(opt_score_total),stroop_difference = mean(stroop_difference))
by_sub_fb <- fb %>%
group_by(workerID) %>%
summarise(respTime_mean = mean(respTime), respTime_sd = sd(respTime),
own_sum_diff = sum(own.cod), other_sum_diff = sum(other.cod),
own_tendency = (sum(own.cod)-sum(other.cod))/(sum(own.cod)+sum(other.cod)),
aq_score = mean(aq_score_subset),
opt_score = mean(opt_score_total),stroop_difference = mean(stroop_difference))
by_sub_lr <- lr %>%
group_by(workerID) %>%
summarise(respTime_mean = mean(respTime), respTime_sd = sd(respTime),
own_sum_diff = sum(own.cod), other_sum_diff = sum(other.cod),
own_tendency = (sum(own.cod)-sum(other.cod))/(sum(own.cod)+sum(other.cod)),
aq_score = mean(aq_score_subset),
opt_score = mean(opt_score_total),stroop_difference = mean(stroop_difference))
# 3.2 Visualizing
View(by_subj_diff)
ggplot(by_subj_diff, aes(own_tendency))+
ggplot_histogramm()
ggplot(by_subj_diff, aes(own_tendency))+
ggplot_histogram()
ggplot(by_subj_diff, aes(own_tendency))+
geom_histogram()
ggplot(by_subj_diff, aes(aq_score, opt_score, size = tendency))+
geom_point()
ggplot(by_subj_diff, aes(aq_score, opt_score, size = own_tendency))+
geom_point()
ggplot(by_subj_diff, aes(own_tendency, opt_score))+
geom_point()
ggplot(by_subj_diff, aes(aq_score, opt_score, color = own_tendency))+
geom_point()
ggplot(by_subj_fb, aes(own_tendency))+
geom_histogram()
by_subj_fb <- fb %>%
group_by(workerID) %>%
summarise(respTime_mean = mean(respTime), respTime_sd = sd(respTime),
own_sum_diff = sum(own.cod), other_sum_diff = sum(other.cod),
own_tendency = (sum(own.cod)-sum(other.cod))/(sum(own.cod)+sum(other.cod)),
aq_score = mean(aq_score_subset),
opt_score = mean(opt_score_total),stroop_difference = mean(stroop_difference))
by_subj_lr <- lr %>%
group_by(workerID) %>%
summarise(respTime_mean = mean(respTime), respTime_sd = sd(respTime),
own_sum_diff = sum(own.cod), other_sum_diff = sum(other.cod),
own_tendency = (sum(own.cod)-sum(other.cod))/(sum(own.cod)+sum(other.cod)),
aq_score = mean(aq_score_subset),
opt_score = mean(opt_score_total),stroop_difference = mean(stroop_difference))
ggplot(by_subj_fb, aes(own_tendency))+
geom_histogram()
ggplot(by_subj_lr, aes(own_tendency))+
geom_histogram()
ggplot(by_subj_fb, aes(own_tendency))+
geom_histogram()
View(by_subj_fb)
View(fb_p)
diff %>%
mutate(prop = round(n_p/n_all,2))%>%
arrange(desc(prop))
ggplot(by_subj_diff, aes(own_tendency))+
geom_histogram()+
ggtitle("Hello")
ggplot(by_subj_diff, aes(own_tendency))+
geom_histogram()+
ggtitle("All different perspective tasks")
ggplot(by_subj_diff, aes(own_tendency))+
geom_histogram()+
ggtitle("Combined different perspective tasks")
ggplot(by_subj_diff, aes(aq_score, opt_score, color = own_tendency))+
geom_point()
p1 <- ggplot(by_subj_diff, aes(own_tendency))+
geom_histogram()+
ggtitle("Combined different perspective tasks")
ggplot(by_subj_diff, aes(aq_score, opt_score, color = own_tendency))+
geom_point()
ggplot(by_subj_diff, aes(own_tendency, opt_score))+
geom_point()
p2 <- (by_subj_fb, aes(own_tendency))+
geom_histogram()+
ggtitle("Only front-back different perspective tasks")
p2 <- ggplot(by_subj_fb, aes(own_tendency))+
geom_histogram()+
ggtitle("Only front-back different perspective tasks")
p3 <- ggplot(by_subj_lr, aes(own_tendency))+
geom_histogram()+
ggtitle("Only left-right different perspective tasks")
grid.arrange(plot1, plot2, ncol=2)
library(gridExtra )
grid.arrange(plot1, plot2, ncol=2)
grid.arrange(p1, p2, ncol=2)
grid.arrange(p1, p2,p3, ncol=3)
by_subj <- clean %>%
group_by(workerID) %>%
summarise(resp_all = mean(respTime), sd_all = sd(respTime), own_sum_all = sum(own.cod), other_sum_all = sum(other.cod))
by_subj_same <- clean %>%
filter(perspective == "same") %>%
group_by(workerID) %>%
summarise(resp_same = mean(respTime), sd_same = sd(respTime), own_sum_same = sum(own.cod), other_sum_same = sum(other.cod))
by_subj <- mutate(by_subj, total_sum_all = own_sum_all+other_sum_all)
by_subj <- mutate(by_subj, direction_all = (own_sum_all-other_sum_all)/total_sum_all)
by_subj_diff <- mutate(by_subj_diff, total_sum_diff = own_sum_diff+other_sum_diff)
by_subj_diff <- mutate(by_subj_diff, direction_diff = (own_sum_diff-other_sum_diff)/total_sum_diff)
by_subj_same <- mutate(by_subj_same, total_sum_same = own_sum_same+other_sum_same)
by_subj_same <- mutate(by_subj_same, direction_same = (own_sum_same-other_sum_same)/total_sum_same)
direction_density <- density(by_subj$direction_all)
direction_density_diff <- density(by_subj_diff$direction_diff)
direction_density_same <- density(by_subj_same$direction_same)
plot(direction_density_diff, ylim = c(0,2), col = "blue")
lines(direction_density, col = "red")
lines(direction_density_same, col = "green")
legend("topleft", legend = c("All trials", "same", "different"),
lwd = 3, col = c("red", "green", "blue"))
grid.arrange(p1, p2,p3, ncol=3)
ggplot(by_subj_diff, aes(aq_score, opt_score, color = own_tendency))+
geom_point()
ggplot(by_subj_diff, aes(own_tendency, opt_score))+
geom_point()
